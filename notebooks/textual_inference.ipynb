{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The notebook is supplementary to the paper [Semantics and Deep Learning](https://lingbuzz.net/lingbuzz/007736).  \n",
        "It is assembled by [Lasha Abzianidz](mailto:lasha.abzianidze@gmail.com)"
      ],
      "metadata": {
        "id": "JxyTOa26ftvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup üõ†Ô∏è\n",
        "\n",
        "Preparing the environment for running demo."
      ],
      "metadata": {
        "id": "fXWh4ncG0_NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import transformers # preinstalled in colab\n",
        "print(f\"transformers ver. = {transformers.__version__}\")\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from nltk.metrics import ConfusionMatrix, scores\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "P_hzZMEdgiBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentencepiece"
      ],
      "metadata": {
        "id": "dtDln6dGWZcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning SemDL package which includes utility functions\n",
        "!rm -fr SemDL # helps to rerun this cell witthout errors, if recloning needed\n",
        "!git clone https://github.com/kovvalsky/SemDL.git"
      ],
      "metadata": {
        "id": "gbyyCf9SxXJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing utility functions from the SemDL package\n",
        "import SemDL.reasoning\n",
        "importlib.reload(SemDL.reasoning) # useful when updating the module files\n",
        "from SemDL.reasoning import gen_syllogism, load_tok_model, predict_nli"
      ],
      "metadata": {
        "id": "2RoOXtyDxxAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading models üì¶\n",
        "\n",
        "We will load a model from the ü§ó[huggingface model](https://huggingface.co/models) hub. With the transformers library this is simple: one needs to provide a huggingface model hub name.  \n",
        "We will load [Nie et al. (2020)](https://aclanthology.org/2020.acl-main.441/)'s natural language inference (NLI) model that is based on the *large* model of [RoBERTa](https://arxiv.org/abs/1907.11692) fine-tuned on four textual inference datasets: [SNLI](https://nlp.stanford.edu/projects/snli/), [MNLI](https://cims.nyu.edu/~sbowman/multinli/), [FEVER-NLI](https://huggingface.co/datasets/pietrolesci/nli_fever), and Adversarial NLI. The model card of the model can be found [here](https://huggingface.co/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli).  \n",
        "Let's load the tokenizer and inference models (we are not going to use GPU as the demo is only about inference without model training).\n",
        "\n"
      ],
      "metadata": {
        "id": "Rrl9MT141z_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli'\n",
        "# the tokenizer model to preprocess the natural language input\n",
        "anli_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "# the model that is responsible for textual inference prediction\n",
        "anli_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "dYlwOmHZg9kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization ü™ì\n",
        "\n",
        "Before predicting an inference label for a sentence pair, we need to tokenize input sentences."
      ],
      "metadata": {
        "id": "BO3rJNlofYEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1, s2 = \"A cat is napping on the mat.\", \"An animal is sleeping.\""
      ],
      "metadata": {
        "id": "ABqJ2HXbhAUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toks1 = anli_tokenizer(s1)\n",
        "print(toks1)"
      ],
      "metadata": {
        "id": "BLxXYtAVgh4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`input_ids` represents IDs of tokens. Note that tokenization can also split the longer or relatively rare words into smaller pieces to prevent the out-of-vocabulary cases.  \n",
        "We can map the token IDs to tokens as follows:"
      ],
      "metadata": {
        "id": "U5ToJ7agi9RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ anli_tokenizer.convert_ids_to_tokens(tok_id) for tok_id in toks1[\"input_ids\"] ]"
      ],
      "metadata": {
        "id": "svPg3-irhbzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see that the input is circumfixed by the sequence tags `<s>` and `</s>`. We see a weird symbol `ƒ†` at the beginning of many tokens. This stands for white space and it is represented by a symbol that has a code point 256 + 32: 32 (code point of a white space) + 256 (just a trick to consistently map invisible characters to the visible ones). For example, this is decision is shared by RoBERTa and GPT-2 tokenizers.  \n",
        "`A` and `apping` have no prefix as they were not preceded with white space.  \n",
        "Probably you also noticed that `napping` is chopped into `n` and `apping`. While this is not ideal, some relatively rare words get such unfair treatment in order to keep the number of tokens tractable and avoid out-of-vocabulary tokens.    \n",
        "\n",
        "To explain the role of `attention_mask`, we need more than one input to the tokenizer. Usually, to make the processing fast, a batch of input is processed in parallel. We will consider here a batch of size 2."
      ],
      "metadata": {
        "id": "CdSzU6e0kQtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization without padding by default\n",
        "pair_toks = anli_tokenizer([s1, s2])\n",
        "print(pair_toks)\n",
        "\n",
        "# tokenization with padding\n",
        "pair_toks_padded = anli_tokenizer([s1, s2], padding=True)\n",
        "print(pair_toks_padded)\n",
        "\n",
        "print(f\"padding symbol is {anli_tokenizer.convert_ids_to_tokens(1)}\")\n",
        "print(f\"contrasting attention_mask for the 2nd input\\n{pair_toks['attention_mask'][1]}\\n{pair_toks_padded['attention_mask'][1]}\")"
      ],
      "metadata": {
        "id": "TKpqpQXVhcHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the calculations in deep learning are carried out with Tensor operations, it is handy to represent a batch of tokenized input as a rectangle matrix. When we set `padding=True` for the tokenizer, then the length of all inputs in a batch is set to the longest input size and shorter inputs are padded with a special `<pad>` token with an ID 1. That's why in the padded version the token IDs of the 2nd sentence are appended with 1s.    \n",
        "`attention_mask` records for each input in the batch which tokens are relevant (marked with 1) and which are due to padding (marked with 0)."
      ],
      "metadata": {
        "id": "2uRI276QniGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that different models might tokenize differently. For example, the [BERT](https://aclanthology.org/N19-1423/)-base tokenizer tokenizes `napping` as `nap` and `ping`.  \n",
        "Let's see this in the example of a BERT-base model. We will use `.tokenize` method to directly obtain a string representation of the tokens."
      ],
      "metadata": {
        "id": "bQfU24UtbyIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that 'bert-base-uncased' is note fine-tuned on an NLI dataset,\n",
        "# however the way tokenizer chops input remains the same with or without fine-tuning\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)"
      ],
      "metadata": {
        "id": "lMnvqm2lccMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer.tokenize(s1)"
      ],
      "metadata": {
        "id": "ulzYO_zkcrTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different language models also might insert different _invisible_ tokens. For example, BERT uses `[CLS]` token to model an entire sequence with a single vector. `CLS` stands for _classification_ (not for _clause_ as linguists might think of :)). `[SEP]` is a sequence separator."
      ],
      "metadata": {
        "id": "7KyvTvNjdRw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ bert_tokenizer.convert_ids_to_tokens(tok_id) for tok_id in bert_tokenizer(s1)[\"input_ids\"] ]"
      ],
      "metadata": {
        "id": "67J-VtATdJu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When classifying two sequences, like NLI and QA tasks require, the class-prediction model takes as input `[CLS]sequence_1[SEP]sequence_2[SEP]`, where sequence_N is a tokenized sequence. That's why two sequences should be fed as two arguments to the corresponding tokenizer.   "
      ],
      "metadata": {
        "id": "85RApPnZuGnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ bert_tokenizer.convert_ids_to_tokens(tok_id) for tok_id in bert_tokenizer(s1, s2)[\"input_ids\"] ]"
      ],
      "metadata": {
        "id": "dsrGVrrdtR1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference prediction üî¨"
      ],
      "metadata": {
        "id": "bDI7j9wAwg4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single problem\n",
        "\n",
        "Let's consider a toy inference problem (which is entailment) with the following premise and hypothesis:"
      ],
      "metadata": {
        "id": "jO9TMskJszKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p, h = \"A cat is napping on the mat.\", \"An animal is sleeping.\""
      ],
      "metadata": {
        "id": "brr9rCIXg1ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we tokenize it together as two sequences\n",
        "# We ask pytorch tensors as output to directly feed the output to the prediction model\n",
        "tokenized_pair = anli_tokenizer(p, h, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "VIQZS7-gyCk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the tokenizer can be used as a dictionary (but it is not a dict type!), hence, it can be fed to the prediction model as a set of parameter-value pairs. We use `**` to convert dict-like objects in parameter-value pairs.  \n",
        "A good thing is that the tokenizer _knows_ what input the corresponding prediction model needs and returns an output that can be directly given to the prediction model."
      ],
      "metadata": {
        "id": "CLksfhrcxXgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(anli_model(**tokenized_pair))\n",
        "print(f\"Mapping positions/indices to inference classes/labels {anli_model.config.id2label}\")"
      ],
      "metadata": {
        "id": "x2zdiCqmz3XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction model returned logits over the possible inference classes. The probability distribution over the classes can be obtained by applying softmax to the logits. The correspondence between the logic positions and classes can be obtained from the configuration of the prediction model.  \n",
        "\n",
        "All these steps are executed under the hood of our wrapper function that returns a dictionary containing the info about the predicted label distribution and the most probable label.  \n",
        "üéâ The model correctly predicts the entailment label for our toy inference problem."
      ],
      "metadata": {
        "id": "T3bOykrDyTrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predict_nli(anli_tokenizer, anli_model, (p, h))\n",
        "print(f\"probability distribution = {prediction['probs']}\")\n",
        "print(f\"predicted label = {prediction['label']}\")"
      ],
      "metadata": {
        "id": "oTRpKj7byTG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a different model, [BART](https://aclanthology.org/2020.acl-main.703/) fine-tuned only on MNLI, on the inference problem."
      ],
      "metadata": {
        "id": "ZLTf5EiS4iAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'facebook/bart-large-mnli'\n",
        "tokenizer, model = load_tok_model(model_name)\n",
        "prediction = predict_nli(tokenizer, model, (p, h))\n",
        "print(prediction['probs'])"
      ],
      "metadata": {
        "id": "CFpGK4ft1onv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction is again entailment with almost perfect probability."
      ],
      "metadata": {
        "id": "Q2R5c-0P486v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_nli(tokenizer, model, (\"John is sleeping\", \"John is sleeping\"))"
      ],
      "metadata": {
        "id": "MSTbrp30-J_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifying syllogisms\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9rpYBtSjreRD_NBlzE5nuA.png\" width=\"64\"/>\n",
        "</div>\n",
        "\n",
        "[Aristotle's syllogisms](https://en.wikipedia.org/wiki/Syllogism) can be regarded as the oldest textual inference problems.\n",
        "\n",
        "The provided `gen_syllogism` function generates all syllogisms.\n",
        "Read the above-cited link to better understand the structure of syllogisms (e.g., categorization of syllogisms based on the figure value).\n",
        "We can also inject desired concepts in the generated syllogisms.\n",
        "In this example we will use nouns for professions/expertise.  \n",
        "Note that the generated syllogisms are labeled with three inference labels.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oF_3SCt-yzfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating a couple of syllogisms\n",
        "for name_label, (p1, p2, c) in gen_syllogism('logicians', 'linguists', 'engineers', figures=\"1\"):\n",
        "    if \"neutral\" not in name_label:\n",
        "        print(f\"{name_label}\\n{p1}\\n{p2}\\n{'':->30}\\n{c}\\n\")"
      ],
      "metadata": {
        "id": "Pso83VrCgqRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate all 256 syllogistic inference problems and classify them with the [ANLI](https://huggingface.co/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli) model."
      ],
      "metadata": {
        "id": "XZH3db9tixAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ref = dict() # keeps gold and predicted labels\n",
        "# looping over the syllogistic problems and classifying them\n",
        "for name_label, (p1, p2, c) in tqdm(gen_syllogism('logicians', 'linguists', 'engineers')):\n",
        "    name, ref_label = name_label.rsplit('-', 1)\n",
        "    pred = predict_nli(anli_tokenizer, anli_model, (f\"{p1}. {p2}.\", f\"{c}.\"))\n",
        "    pred_ref[name] = pred['label'], ref_label"
      ],
      "metadata": {
        "id": "IGOhrDEv8zM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw confusion matrix\n",
        "preds, refs = zip(*pred_ref.values())\n",
        "cm = metrics.confusion_matrix(refs, preds)\n",
        "draw_cm = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = ['contradiction', 'entailment', 'neutral'])\n",
        "draw_cm.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YN59hyI4vx1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy\n",
        "acc = metrics.accuracy_score(refs, preds)\n",
        "print(f\"Accuracy = {acc}\")"
      ],
      "metadata": {
        "id": "0VmIxAN1lZlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the model performs poorly on the syllogisms. It is known that large language model-based inference systems are not good at logical reasoning. Our results confirm this fact.\n",
        "\n",
        "For analysis, below we print interesting cases of syllogism: problems that are entailment but were predicted as contradiction.  "
      ],
      "metadata": {
        "id": "L3zQkhWkxIwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis\n",
        "for name_label, (p1, p2, c) in gen_syllogism('logicians', 'linguists', 'engineers'):\n",
        "    name, ref_label = name_label.rsplit('-', 1)\n",
        "    pred, ref = pred_ref[name]\n",
        "    if pred != ref and {ref, pred} == {\"entailment\", \"contradiction\"}:\n",
        "        print(f\"{name}\\t{ref.upper()}\\t{pred}\\n{p1}\\n{p2}\\n{'':->30}\\n{c}\\n\")"
      ],
      "metadata": {
        "id": "9VsrZjqO5opr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice neural models are run on GPUs and on the batched input. In this way prediction and training procedures are lot faster than on a CPU."
      ],
      "metadata": {
        "id": "wQZwuMLL2Shv"
      }
    }
  ]
}